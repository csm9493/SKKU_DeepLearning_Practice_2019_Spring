{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice3. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as pickle\n",
    "import time\n",
    "\n",
    "# set default plot options\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up input preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_CIFAR10_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_tr, X_te, Y_te, mean_img = get_CIFAR10_data()\n",
    "print ('Train data shape : %s,  Train labels shape : %s' % (X_tr.shape, Y_tr.shape))\n",
    "print ('Test data shape : %s,  Test labels shape : %s' % (X_te.shape, Y_te.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*10000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_tr[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_tr[idx]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Softmax 함수를 생성하라.\n",
    "    NaN가 생성되지 않도록 구현 할 것(오버플로우 방지)\n",
    "    \n",
    "    Inputs : \n",
    "        - x : (N,D) 차원의 벡터\n",
    "    \n",
    "    Output : \n",
    "        - softmax_output : (N,D) 차원의 Softmax 결과\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    softmax_output = None\n",
    "    \n",
    "    #########################################################################################################\n",
    "    #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "    #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "    #########################################################################################################\n",
    "    \n",
    "    return softmax_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax 구현 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = np.array([[2060,2000,2080]])\n",
    "print (temp_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_result1 = softmax(temp_x)\n",
    "print (softmax_result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    반드시 array([[2.06115362e-09, 1.80485138e-35, 9.99999998e-01]]) 과 유사한 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.sum(softmax_result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 0.9999999999999999 과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = np.array([[2060,2000,2080],[1010,1020,1030]])\n",
    "print (temp_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_result2 = softmax(temp_x)\n",
    "print (softmax_result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[2.06115362e-09, 1.80485138e-35, 9.99999998e-01],  \n",
    "                [2.06106005e-09, 4.53978686e-05, 9.99954600e-01]]) 과 유사한 결과를 얻어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.sum(softmax_result2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 [1. 1.] 과 같은 결과를 얻어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy_loss(softmax_output, target, weights, regularization):\n",
    "    \n",
    "    \"\"\"\n",
    "    cross_entropy_loss 함수를 생성하라.\n",
    "    delta를 이용하여 log값의 input으로 0이 들어가지 않도록 대처하라\n",
    "    Weights를 이용한 L2 Regularization을 고려하여 구현하라\n",
    "    L2 Regularization은 0.5를 곱하여 계산하라 (미분 할 때 계산적 이점)\n",
    "    \n",
    "    Inputs : \n",
    "        - softmax_output : Softmax의 결과로 넘어오는 (N, D) 차원의 벡터\n",
    "        - target : (N, # of labels) 차원의 벡터 (One-hot encoding)\n",
    "        - weights : NN 모델에 존재하는 모든 Weight 값(Dictionary 형태)\n",
    "        - regularization : regularization를 결정하는 0과 1사이의 수\n",
    "    \n",
    "    Output : \n",
    "        - loss : scalar인 loss 값\n",
    "    \"\"\"\n",
    "    \n",
    "    delta = 1e-9\n",
    "    batch_size = target.shape[0]\n",
    "    data_loss = 0\n",
    "    reg_loss = 0\n",
    "    loss = None\n",
    "    \n",
    "    #########################################################################################################\n",
    "    #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "    #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "    #########################################################################################################\n",
    "    \n",
    "    loss = data_loss + reg_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross entropy loss 구현 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score0 = np.array([[0.0, 0.0, 0.0]])\n",
    "temp_target0 = np.array([[0,1,0]])\n",
    "print (temp_score0.shape)\n",
    "print (temp_target0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weight1 = np.array([[0.1,0.2,0.3],[-0.5,0.3,-0.8]])\n",
    "temp_weight2 = np.array([[0.9,-0.5,0.3],[0.9,0.6,-0.8]])\n",
    "\n",
    "weights = {}\n",
    "weights['W1'] = (temp_weight1)\n",
    "weights['W2'] = (temp_weight2)\n",
    "\n",
    "reg_term = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss(temp_score0, temp_target0, weights, reg_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 20.72530583694641 과 유사한 결과를 얻어야 함. (NaN이 나오면 안됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score1 = np.array([[0.1, 0.3, 0.6]])\n",
    "temp_target1 = np.array([[0,1,0]])\n",
    "print (temp_score1.shape)\n",
    "print (temp_target1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss(temp_score1, temp_target1 , weights, reg_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 1.2060128009926026 과 유사한 결과를 얻어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score2 = np.array([[0.1, 0.3, 0.6],[0.2,0.4,0.4],[0.9,0.05,0.05]])\n",
    "temp_target2 = np.array([[0,1,0],[0,0,1],[1,0,0]])\n",
    "print (temp_score2.shape)\n",
    "print (temp_target2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss(temp_score2, temp_target2 , weights, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 0.7439146816378243 과 유사한 결과를 얻어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OutputLayer:\n",
    "    \n",
    "    \"\"\"\n",
    "    Softmax를 이용한 Cross-entropy loss를 계산하는 Ouput Layer class를 생성하라.\n",
    "    앞서 생성한 softmax() 및 cross_entropy_loss를 이용하여 구현 할 것\n",
    "    forward, backward의 계산과정을 생각하여 구현할 것\n",
    "    backpropagation에서 softmax의 output과 target label의 정보가 이용됨을 유념하여 구현할 것\n",
    "    \n",
    "    forward() : \n",
    "        - x : Forward propagation 과정에서 이전 레이어로부터 넘어오는 (N,D) 차원의 벡터\n",
    "        - y : (N, # of Label) 차원의 벡터 \n",
    "        - return : softmax loss\n",
    "    \n",
    "    backward() : \n",
    "        - dout : backpropagation 과정에서 이전 레이어로부터 넘어오는 오는 delta 값, output layer 이므로 delta = 1\n",
    "        - return : dx\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, weights, regularization):\n",
    "        self.loss = None           # loss value\n",
    "        self.output_softmax = None # Output of softmax\n",
    "        self.target_label = None   # Target label (one-hot vector)\n",
    "        self.weights = weights\n",
    "        self.regularization = regularization\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "    \n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "    \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        bt_size = self.target_label.shape[0]\n",
    "        dx = None\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlayer = OutputLayer(weights, reg_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x1 = np.array([[3, -10, 0.7]])\n",
    "temp_t1 = np.array([[0,1,0]])\n",
    "print (temp_x1.shape)\n",
    "print (temp_t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlayer.forward(temp_x1, temp_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 13.097100867144416 값과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlayer.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[ 0.90887517, -0.99999795,  0.09112277]]) 값과 유사한 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x2 = np.array([[3, -10, 0.7],[9,15,-1],[-5,1,-1]])\n",
    "temp_t2 = np.array([[0,1,0],[1,0,0],[0,0,1]])\n",
    "print (temp_x2.shape)\n",
    "print (temp_t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlayer.forward(temp_x2, temp_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 7.077588386844261 값과 유사한 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlayer.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[ 3.02958391e-01, -3.33332649e-01,  3.03742579e-02],  \n",
    "                 [-3.32509126e-01,  3.32509088e-01,  3.74189683e-08],  \n",
    "                 [ 7.26173786e-04,  2.92959414e-01, -2.93685588e-01]]) 값과 유사한 결과를 얻어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \n",
    "    \"\"\"\n",
    "    ReLU를 구현하라\n",
    "    forward, backward의 계산과정을 생각하여 구현할 것\n",
    "    backpropagation에서 forward 과정에서 사용 된 mask 정보가 사용됨을 유념하여 구현할 것\n",
    "    \n",
    "    forward() : \n",
    "        - x : Forward propagation 과정에서 이전 레이어로부터 넘어오는 (N,D) 차원의 벡터\n",
    "        - return : ReLU output\n",
    "    \n",
    "    backward() : \n",
    "        - dout : backpropagation 과정에서 이전 레이어로부터 넘어오는 오는 delta 값\n",
    "        - return : dx\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.out = None\n",
    "    \n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "    \n",
    "        dx = None\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x1 = np.array([[3, -10, 0.7]])\n",
    "temp_x2 = np.array([[-10,1,0]])\n",
    "print (temp_x1.shape)\n",
    "print (temp_x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu.forward(temp_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[3. , 0. , 0.7]]) 값과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu.backward(temp_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[-10,   0,   0]]) 값과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x3 = np.array([[3, -10, 0.7],[9,15,-1],[-5,1,-1]])\n",
    "temp_x4 = np.array([[3,5,-10],[5,-4,2],[-3,-5,3]])\n",
    "print (temp_x3.shape)\n",
    "print (temp_x4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu.forward(temp_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[ 3. ,  0. ,  0.7],\n",
    "                 [ 9. , 15. ,  0. ],\n",
    "                 [ 0. ,  1. ,  0. ]]) 값과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu.backward(temp_x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[  3,   0, -10],\n",
    "                 [  5,  -4,   0],\n",
    "                 [  0,  -5,   0]]) 값과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \n",
    "    \"\"\"\n",
    "    Sigmoid 구현하라\n",
    "    forward, backward의 계산과정을 생각하여 구현할 것\n",
    "    backpropagation 과정에서 forward 과정에서 나온 계산 결과가 사용됨을 유념하여 구현할 것\n",
    "    \n",
    "    forward() : \n",
    "        - x : Forward propagation 과정에서 이전 레이어로부터 넘어오는 (N,D) 차원의 벡터\n",
    "        - return : Sigmoid output\n",
    "    \n",
    "    backward() : \n",
    "        - dout : backpropagation 과정에서 이전 레이어로부터 넘어오는 오는 delta 값\n",
    "        - return : dx\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \n",
    "        dx = None\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #############################################################################################s############\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x1 = np.array([[3, -10, 0.7]])\n",
    "temp_t1 = np.array([[0,1,0]])\n",
    "print (temp_x1.shape)\n",
    "print (temp_t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigmoid.forward(temp_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[9.52574127e-01, 4.53978687e-05, 6.68187772e-01]]) 값과 유사한 결과를 얻어야 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid.backward(temp_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[ 0.13552998, -0.00045396,  0.15519901]]) 값과 유사한 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x2 = np.array([[3, -10, 0.7],[9,15,-1],[-5,1,-1]])\n",
    "temp_t2 = np.array([[0,1,0],[1,0,0],[0,0,1]])\n",
    "print (temp_x2.shape)\n",
    "print (temp_t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid.forward(temp_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[9.52574127e-01, 4.53978687e-05, 6.68187772e-01],  \n",
    "                 [9.99876605e-01, 9.99999694e-01, 2.68941421e-01],  \n",
    "                 [6.69285092e-03, 7.31058579e-01, 2.68941421e-01]]) 값과 유사한 결과를 얻어야 함         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid.backward(temp_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[ 1.35529979e-01, -4.53958077e-04,  1.55199011e-01],  \n",
    "                 [ 1.11041415e-03,  4.58853200e-06, -1.96611933e-01],  \n",
    "                 [-3.32402834e-02,  1.96611933e-01, -1.96611933e-01]]) 값과 유사한 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    \n",
    "    \"\"\"\n",
    "    Affine 계층을 구현하라\n",
    "    Affine 계층이란 Y = np.dot(X, W) + B (X는 입력, W는 가중치, B는 바이어스)와 같은 한 뉴런의 가중치합을 계산하는 것\n",
    "    forward, backward의 계산과정을 생각하여 구현할 것\n",
    "    Backward 과정에서 W,b 그리고 x 까지 모두 사용됨을 유의하여 구현할 것\n",
    "    \n",
    "    forward() : \n",
    "        - x : Forward propagation 과정에서 이전 레이어로부터 넘어오는 (N,D) 차원의 벡터\n",
    "        - return : Affine output\n",
    "    \n",
    "    backward() : \n",
    "        - dout : backpropagation 과정에서 이전 레이어로부터 넘어오는 오는 delta 값\n",
    "        - return : dx\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = None\n",
    "    \n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \n",
    "        dx = None\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_W = np.array([[0.2, -0.3, 0.6], [-0.9, 0.1, -0.4]])\n",
    "temp_b = np.array([[0.2, -0.3, 0.6]])\n",
    "print (temp_W.shape)\n",
    "print (temp_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = np.array([[0.2, -0.3], [-0.9, 0.1]])\n",
    "print (temp_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = Affine(temp_W, temp_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine.forward(temp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[ 0.51, -0.39,  0.84],  \n",
    "                 [-0.07, -0.02,  0.02]]) 값과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_t = np.array([[0.1, 0.5, -0.8], [0.4, 0.7, -0.2]])\n",
    "affine.backward(temp_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[-0.61,  0.28],\n",
    "                  [-0.25, -0.21]]) 값과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine.dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([[-0.34, -0.53,  0.02],\n",
    "                  [ 0.01, -0.08,  0.22]]) 값과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    반드시 array([ 0.5,  1.2, -1. ]) 값과 같은 결과를 얻어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. TwoLayerNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) TwoLayerNet 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from utils import numerical_gradient\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    __init__() : \n",
    "        - Weight 및 bias를 initialization 하는 함수\n",
    "        - Initialization 된 Weight와 bias를 이용하여 계층을 생성하는 함수\n",
    "    \n",
    "    predict() : \n",
    "        - Input data(x)에 대해서 Neural network의 forward propagtion을 수행하는 함수\n",
    "        \n",
    "    loss() : \n",
    "        - Input data(x)에 대해서 Neural network의 forward propagtion을 수행한 결과를 이용하여 Loss를 계산하는 함수\n",
    "        \n",
    "    accuracy() :\n",
    "        - Input data(x)의 결과와 True label(y)을 이용하여 Accuracy를 구하는 함수\n",
    "        \n",
    "    numerical_gradient() :\n",
    "        - Input data(x)와 True label(y)을 이용하여 Numerical_gradient를 구하는 함수\n",
    "        - Backpropagation 방법과의 비교를 위해 사용\n",
    "        \n",
    "    gradient():\n",
    "        - Input data(x)와 True label(y)을 이용하여 Backpropagation을 수행하는 함수\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01, regularization = 0.0):\n",
    "\n",
    "        # 가중치 초기화\n",
    "        # Weight initialization\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.weights = {}\n",
    "        self.weights['W1'] = self.params['W1']\n",
    "        self.weights['W2'] = self.params['W2']\n",
    "        \n",
    "        self.reg = regularization\n",
    "\n",
    "        # 계층 생성\n",
    "        # Layer generation\n",
    "        self.layers = OrderedDict() # infromation about OrderedDict (https://pymotw.com/2/collections/ordereddict.html)\n",
    "        \n",
    "        #########################################################################################################\n",
    "        # TwoLayerNet을 구현하라                                                                                 \n",
    "        # Neural Network의 구조는 다음과 같이 구현할 것                                                           \n",
    "        # [ Input => Fully Connected => ReLU => Fully Connected(OutputLayer) ]\n",
    "        # 구현시 위 과정에서 생성한 Class(Affine, ReLU)들 및 Weight initialization 과정에서 생성한 weight를 이용하여 구현할 것\n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "        # self.layers['Affine1'] = None\n",
    "        # self.layers['Relu1'] = None\n",
    "        # self.layers['Affine2'] = None\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "        self.lastLayer = OutputLayer(self.weights, self.reg)\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        score = self.predict(x)\n",
    "        return self.lastLayer.forward(score, y)\n",
    "\n",
    "    \n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "\n",
    "        score = self.predict(x)\n",
    "        score = np.argmax(score, axis=1)\n",
    "        if y.ndim != 1 : y = np.argmax(y, axis=1)\n",
    "        accuracy = np.sum(score == y) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, y):\n",
    "\n",
    "        loss_W = lambda W: self.loss(x, y)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "            \n",
    "        return grads\n",
    "\n",
    "        \n",
    "\n",
    "    def gradient(self, x, y):\n",
    "\n",
    "        # forward\n",
    "        self.loss(x, y)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        \n",
    "        if self.reg != 0.0:\n",
    "            \n",
    "            #########################################################################################################\n",
    "            # 각 Weight의 Gradient를 구할 때 Regularization에 의한 영향을 구현하라\n",
    "            #########################################################################################################\n",
    "            #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "            #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "            #########################################################################################################\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Numerical gradient vs Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "network = TwoLayerNet(input_size=3072, hidden_size=10, output_size=10, regularization = 0.0001)\n",
    "\n",
    "x_batch = X_tr[:3]\n",
    "t_batch = Y_tr[:3]\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "targets = t_batch.reshape(-1)\n",
    "t_batch = np.eye(nb_classes)[targets]\n",
    "\n",
    "start_time = time.time() \n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "print(\"[grad_backprop] running time(sec) : \" +str(time.time() - start_time))\n",
    "\n",
    "start_time = time.time() \n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "print(\"[grad_numerical] running time(sec) : \"+str(time.time() - start_time))\n",
    "\n",
    "print ()\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Running time은 grad_backprop를 사용했을 때 더 빨라야 함\n",
    "\n",
    "    Gradient 값의 차이는  \n",
    "    W1:7.965420308015262e-07  \n",
    "    b1:3.8788064893238025e-11  \n",
    "    W2:8.089449817202143e-07  \n",
    "    b2:1.528478527079713e-09  결과와 유사하게 차이가 적어야 함\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cifar-10 dataset의 Label을 One-hot vector 형태로 바꿔주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "targets = Y_tr.reshape(-1)\n",
    "Y_tr_onehot = np.eye(nb_classes)[targets]\n",
    "\n",
    "targets = Y_te.reshape(-1)\n",
    "Y_te_onehot = np.eye(nb_classes)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_size_=3072\n",
    "hidden_size_=1024\n",
    "output_size_=10\n",
    "regularization_ = 0.0001\n",
    "\n",
    "network = TwoLayerNet(input_size=input_size_, hidden_size=hidden_size_, output_size=output_size_, regularization = regularization_)\n",
    "\n",
    "iters_num = 2000\n",
    "train_size = X_tr.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list_two = []\n",
    "train_acc_list_two = []\n",
    "test_acc_list_two = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "\n",
    "for i in range(iters_num):\n",
    "\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_tr[batch_mask]\n",
    "    t_batch = Y_tr_onehot[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch) \n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    for key in ('W1','W2'):\n",
    "        network.weights[key] = network.params[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list_two.append(loss)    \n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(X_tr, Y_tr_onehot)\n",
    "        test_acc = network.accuracy(X_te, Y_te_onehot)\n",
    "        train_acc_list_two.append(train_acc)\n",
    "        test_acc_list_two.append(test_acc)\n",
    "\n",
    "        print(\"Epoch : \",i / iter_per_epoch + 1, \"Training acc : \", round(train_acc,2), \"Test acc : \", round(test_acc,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "plt.plot(train_acc_list_two, label='train acc, two layer NN')\n",
    "plt.plot(test_acc_list_two, label='test acc, two layer NN')\n",
    "plt.title('training result on two layer NN', fontsize=15)\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('epoch', fontsize=15)\n",
    "plt.legend(loc='lower right', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. ThreeLayerNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TwoLayerNN 코드를 참고하여 ThreeLayerNN을 구현하고 Training/Test 하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    __init__() : \n",
    "        - Weight 및 bias를 initialization 하는 함수\n",
    "        - Initialization 된 Weight와 bias를 이용하여 계층을 생성하는 함수\n",
    "    \n",
    "    predict() : \n",
    "        - Input data(x)에 대해서 Neural network의 forward propagtion을 수행하는 함수\n",
    "        \n",
    "    loss() : \n",
    "        - Input data(x)에 대해서 Neural network의 forward propagtion을 수행한 결과를 이용하여 Loss를 계산하는 함수\n",
    "        \n",
    "    accuracy() :\n",
    "        - Input data(x)의 결과와 True label(y)을 이용하여 Accuracy를 구하는 함수\n",
    "        \n",
    "    numerical_gradient() :\n",
    "        - Input data(x)와 True label(y)을 이용하여 Numerical_gradient를 구하는 함수\n",
    "        - Backpropagation 방법과의 비교를 위해 사용\n",
    "    gradient():\n",
    "        - Input data(x)와 True label(y)을 이용하여 Backpropagation을 수행하는 함수\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, weight_init_std = 0.01, regularization = 0.0):\n",
    "\n",
    "        \n",
    "        #########################################################################################################\n",
    "        # TwoLayerNet을 응용하여 ThreeLayerNet 구현하라\n",
    "        # Neural Network의 구조는 다음과 같이 구현할 것\n",
    "        #[ Input => Fully Connected => ReLU => Fully Connected => ReLU => Fully Connected(OutputLayer) ]\n",
    "        #구현시 위 과정에서 생성한 Class를 이용하여 구현할 것\n",
    "        #* Hidden Layer가 증가함에 따라 바꿔야하는 요소들(Weight 및 bias 추가, Hidden Layer의 weight 수, Weight update 등)을\n",
    "        #충분히 고려하여 구현할 것 *\n",
    "        # Hidden Layer의 변수로 hidden_size1, hidden_size2를 사용할 것\n",
    "        \n",
    "        \n",
    "        # Implement ThreeLayerNet applying TwoLayerNet\n",
    "        # Implement Neural Network structure as follow:\n",
    "        #[ Input => Fully Connected => ReLU => Fully Connected => ReLU => Fully Connected(OutputLayer) ]\n",
    "        # Implement using the Class previously made\n",
    "        # * Consider the elements to be changed as Hidden Layer increase (e.g. addition of Weight and bias, number of weight in Hidden Layers, \n",
    "        #  Weight update) *\n",
    "        # Use hidden_size1 and hidden_size2 as variables of Hidden Layer\n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def loss(self, x, y):        \n",
    "        score = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(score, y)\n",
    "\n",
    "    \n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        score = self.predict(x)\n",
    "        score = np.argmax(score, axis=1)\n",
    "        if y.ndim != 1 : y = np.argmax(y, axis=1)\n",
    "        accuracy = np.sum(score == y) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "        \n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        # forward\n",
    "        self.loss(x, y)\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        #########################################################################################################\n",
    "        # 각 Weight 및 bias의 Gradient를 구하는 부분을 구현하라\n",
    "        # Implement the part obtaining Gradient of each Weight and bias\n",
    "        #########################################################################################################\n",
    "        #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "        #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "        #########################################################################################################\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThreeLayerNet Training / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input_size=3072\n",
    "_hidden_size1=1024\n",
    "_hidden_size2=1024\n",
    "_output_size=10\n",
    "_regularization= 0.0001\n",
    "\n",
    "network = ThreeLayerNet(input_size=_input_size, hidden_size1=_hidden_size1, hidden_size2 = _hidden_size2, output_size = _output_size, regularization = _regularization)\n",
    "\n",
    "iters_num = 2000\n",
    "train_size = X_tr.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list_three = []\n",
    "train_acc_list_three = []\n",
    "test_acc_list_three = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "\n",
    "for i in range(iters_num):\n",
    "\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_tr[batch_mask]\n",
    "    t_batch = Y_tr_onehot[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    #########################################################################################################\n",
    "    # Gradient를 이용하여 Weight 및 bias를 update 하는 부분을 구현하라.\n",
    "    # learning_rate 변수를 사용하여 구현할 것\n",
    "    \n",
    "    # Implement the Weight and bias updating part using Gradient.\n",
    "    # Implement using learng_rate variable.\n",
    "    #########################################################################################################\n",
    "    #------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "        \n",
    "    #-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "    #########################################################################################################\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list_three.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(X_tr, Y_tr_onehot)\n",
    "        test_acc = network.accuracy(X_te, Y_te_onehot)\n",
    "        train_acc_list_three.append(train_acc)\n",
    "        test_acc_list_three.append(test_acc)\n",
    "\n",
    "        print(\"Epoch : \",i / iter_per_epoch + 1, \"Training acc : \", round(train_acc,2), \"Test acc : \", round(test_acc,2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "plt.plot(train_acc_list_three, label='train acc, three layer NN')\n",
    "plt.plot(test_acc_list_three, label='test acc, three layer NN')\n",
    "plt.title('training result on three layer NN', fontsize=15)\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('epoch', fontsize=15)\n",
    "plt.legend(loc='lower right', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "plt.plot(train_acc_list_two, label='train acc, two layer NN')\n",
    "plt.plot(train_acc_list_three, label='train acc, three layer NN')\n",
    "plt.plot(test_acc_list_two, label='test acc, two layer NN')\n",
    "plt.plot(test_acc_list_three, label='test acc, three layer NN')\n",
    "plt.title('training results on two and three layer NN', fontsize=15)\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('epoch', fontsize=15)\n",
    "plt.legend(loc='lower right', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
